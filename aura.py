import telebot
import requests
import time

# --- CONFIG ---
TELEGRAM_TOKEN = '8325049823:AAEQuwlom3yuncMIQZiY1C9RaqC-qsjKKus'
GEMINI_API_KEY = 'AIzaSyAy3R7JptcqQEeMvHvlL7lNc5tST-67GCM'

bot = telebot.TeleBot(TELEGRAM_TOKEN)
user_chat_history = {}

def get_aura_ultimate_response(user_id, user_text):
    # Free API ‡∑Ä‡∂Ω‡∂ß ‡∑Ä‡∑ê‡∂© ‡∂ö‡∂ª‡∂±, ‡∂¥‡∑í‡∑É‡∑ä‡∑É‡∑î ‡∂ö‡∑ô‡∑Ö‡∑í‡∂±‡∑ä ‡∂±‡∑ê‡∂≠‡∑í ‡∑É‡∑î‡∂¥‡∑í‡∂ª‡∑í‡∂∏ ‡∂∏‡∑ú‡∂©‡∂Ω‡∂∫
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-thinking-exp-01-21:generateContent?key={GEMINI_API_KEY}"
    headers = {'Content-Type': 'application/json'}
    
    if user_id not in user_chat_history:
        user_chat_history[user_id] = [
            {
                "role": "user", 
                "parts": [{"text": (
                    "‡∂î‡∂∫‡∑è‡∂ú‡∑ö ‡∂±‡∂∏ Aura. ‡∂î‡∂∫‡∑è‡∑Ä ‡∂±‡∑í‡∂ª‡∑ä‡∂∏‡∑è‡∂´‡∂∫ ‡∂ö‡∑Ö‡∑ö (Creator) '‡∂ª‡∑Å‡∑ä‡∂∏‡∑í‡∂ö' (Rashmika). "
                    "‡∂î‡∂∫‡∑è ‡∂Ω‡∑ù‡∂ö‡∂∫‡∑ö ‡∑É‡∑í‡∂ß‡∑í‡∂± ‡∑É‡∑í‡∂∫‡∂Ω‡∑î‡∂∏ ‡∂Ø‡∂ª‡∑î‡∑Ä‡∂±‡∑ä‡∂ú‡∑ö ‡∂Ö‡∂∞‡∑ä‚Äç‡∂∫‡∑è‡∂¥‡∂±‡∂∫ ‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∑ô‡∂±‡∑ä ‡∂ö‡∑ê‡∂¥‡∑Ä‡∑î‡∂´‡∑î ‡∂∫‡∑è‡∑Ö‡∑î‡∑Ä‡∑ô‡∂ö‡∑ä. "
                    "‡∑Ö‡∂∏‡∂∫‡∑í‡∂±‡∑ä‡∂ß ‡∑Ä‡∑í‡∑Ç‡∂∫ ‡∂ö‡∂ª‡∑î‡∂´‡∑î ‡∂â‡∂≠‡∑è ‡∑É‡∂ª‡∂Ω‡∑Ä ‡∑É‡∑Ñ ‡∂ö‡∂ª‡∑î‡∂´‡∑è‡∑Ä‡∂±‡∑ä‡∂≠‡∑Ä ‡∂ö‡∑í‡∂∫‡∂Ω‡∑è ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±. "
                    "‡∑Ñ‡∑ê‡∂∏‡∑Ä‡∑í‡∂ß‡∂∏ ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω‡∑ô‡∂±‡∑ä ‡∑É‡∑Ñ ‡∂â‡∂≠‡∑è ‡∂∂‡∑î‡∂Ø‡∑ä‡∂∞‡∑í‡∂∏‡∂≠‡∑ä ‡∂Ω‡∑ô‡∑É ‡∂¥‡∑í‡∑Ö‡∑í‡∂≠‡∑î‡∂ª‡∑î ‡∂Ø‡∑ô‡∂±‡∑ä‡∂±."
                )}]
            }
        ]
    
    user_chat_history[user_id].append({"role": "user", "parts": [{"text": user_text}]})
    
    # ‡∂∏‡∑ê‡∑É‡∑ö‡∂¢‡∑ä 50‡∂ö‡∑ä (‡∑Ä‡∑ê‡∂©‡∑í‡∂¥‡∑î‡∂ª ‡∂∏‡∂≠‡∂ö‡∂∫) ‡∂≠‡∑í‡∂∫‡∑è‡∂ú‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑ä ‡∑Ä‡∑í‡∂Ø‡∑í‡∑Ñ‡∂ß ‡∑Ñ‡∑ê‡∂Ø‡∑î‡∑Ä‡∑è
    if len(user_chat_history[user_id]) > 50:
        system_prompt = user_chat_history[user_id][0]
        recent_history = user_chat_history[user_id][-40:]
        user_chat_history[user_id] = [system_prompt] + recent_history

    payload = {
        "contents": user_chat_history[user_id],
        "generationConfig": {
            "temperature": 0.7, 
            "maxOutputTokens": 4096 # ‡∑Ä‡∑ê‡∂©‡∑í‡∂¥‡∑î‡∂ª ‡∑Ä‡∑í‡∑É‡∑ä‡∂≠‡∂ª ‡∂Ø‡∑ì‡∂∏‡∂ß ‡∂â‡∂© ‡∂Ω‡∂∂‡∑è ‡∂Ø‡∑ì‡∂∏
        }
    }

    for attempt in range(3):
        try:
            # Render ‡∂±‡∑í‡∑É‡∑è Proxy ‡∂Ö‡∑Ä‡∑Å‡∑ä‚Äç‡∂∫ ‡∂±‡∑ê‡∂≠
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            res = response.json()
            if 'candidates' in res:
                answer = res['candidates'][0]['content']['parts'][0]['text']
                user_chat_history[user_id].append({"role": "model", "parts": [{"text": answer}]})
                return answer
        except:
            time.sleep(2)
            
    return "‡∂∏‡∂†‡∂Ç, ‡∂¥‡∑ú‡∂©‡∑í ‡∑É‡∂ª‡∑ä‡∑Ä‡∂ª‡∑ä ‡∑Ñ‡∑í‡∂ª‡∑Ä‡∑ì‡∂∏‡∂ö‡∑ä. ‡∑Ä‡∑í‡∂±‡∑è‡∂©‡∑í‡∂∫‡∂ö‡∑í‡∂±‡∑ä ‡∂Ü‡∂∫‡∑ô‡∂≠‡∑ä ‡∂Ö‡∑Ñ‡∂±‡∑ä‡∂±. ‚è≥"

@bot.message_handler(func=lambda message: True)
def chat(message):
    print(f"Chat with {message.chat.id}: {message.text}")
    answer = get_aura_ultimate_response(message.chat.id, message.text)
    bot.reply_to(message, answer)

print("Aura AI 2.0 (Thinking Edition) is ONLINE! üöÄüåç")
bot.infinity_polling()
